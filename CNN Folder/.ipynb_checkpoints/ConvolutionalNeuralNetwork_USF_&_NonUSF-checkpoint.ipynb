{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In this line , We have imported all the packages that will be required further.\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import sys\n",
    "import inspect\n",
    "import datetime\n",
    "import matplotlib.pyplot as plt \n",
    "from datetime import datetime\n",
    "from scipy import misc\n",
    "from scipy import ndimage, misc\n",
    "from sklearn.datasets import fetch_mldata\n",
    "from PIL import Image\n",
    "import matplotlib\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Conv1D,Conv2D, MaxPooling2D\n",
    "from keras.layers import Flatten, Dense, Activation, Dropout\n",
    "#from keras import backend as K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is about getting the current directory path. \n",
    "# So, we are not required to chnage the path for dataset. It will automatically take the path for dataset.\n",
    "path = os.getcwd() + \"\\\\dataset_USF_NonUSF_Train\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is about getting the current directory path. \n",
    "# So, we are not required to chnage the path for dataset. It will automatically take the path for dataset.\n",
    "path1 = os.getcwd() + \"\\\\dataset_USF_NonUSF_Test\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\asus\\Anaconda3\\Lib\\site-packages\\ipykernel_launcher.py:6: DeprecationWarning: `imread` is deprecated!\n",
      "`imread` is deprecated in SciPy 1.0.0.\n",
      "Use ``matplotlib.pyplot.imread`` instead.\n",
      "  \n",
      "C:\\Users\\asus\\Anaconda3\\Lib\\site-packages\\ipykernel_launcher.py:7: DeprecationWarning: `imresize` is deprecated!\n",
      "`imresize` is deprecated in SciPy 1.0.0, and will be removed in 1.2.0.\n",
      "Use ``skimage.transform.resize`` instead.\n",
      "  import sys\n"
     ]
    }
   ],
   "source": [
    "# In this line of code, It will read the Images file from Train dataset and will resize each image file.\n",
    "# For resizing the image file, we are considering converting to 784 pixels each file, so that we can reduce the computation time.\n",
    "\n",
    "images = []\n",
    "X_Train = []\n",
    "for root, dirnames, filenames in os.walk(path):\n",
    "    for filename in filenames:\n",
    "        filepath = os.path.join(root, filename)\n",
    "        image = ndimage.imread(filepath, mode=\"1\")\n",
    "        image_resized = misc.imresize(image, (28,28))\n",
    "        X_Train.append(image_resized)\n",
    "        #image_resized = np.reshape(image_resized, (np.product(image_resized.shape)))\n",
    "        #images.append(image_resized) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\asus\\Anaconda3\\Lib\\site-packages\\ipykernel_launcher.py:6: DeprecationWarning: `imread` is deprecated!\n",
      "`imread` is deprecated in SciPy 1.0.0.\n",
      "Use ``matplotlib.pyplot.imread`` instead.\n",
      "  \n",
      "C:\\Users\\asus\\Anaconda3\\Lib\\site-packages\\ipykernel_launcher.py:7: DeprecationWarning: `imresize` is deprecated!\n",
      "`imresize` is deprecated in SciPy 1.0.0, and will be removed in 1.2.0.\n",
      "Use ``skimage.transform.resize`` instead.\n",
      "  import sys\n"
     ]
    }
   ],
   "source": [
    "# In this line of code, It will read the Images file from Test dataset and will resize each image file.\n",
    "# For resizing the image file, we are considering converting to 784 pixels each file, so that we can reduce the computation time.\n",
    "\n",
    "images = []\n",
    "X_Test = []\n",
    "for root, dirnames, filenames in os.walk(path1):\n",
    "    for filename in filenames:\n",
    "        filepath = os.path.join(root, filename)\n",
    "        image = ndimage.imread(filepath, mode=\"1\")\n",
    "        image_resized = misc.imresize(image, (28,28))\n",
    "        X_Test.append(image_resized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(350, 28, 28)"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# To check the training dataset shape\n",
    "np.asarray(X_Train).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(70, 28, 28)"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# To check the testing dataset shape\n",
    "np.asarray(X_Test).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For counting total number of Images in Folder\n",
    "count = 0\n",
    "for i in X_Train:\n",
    "    count+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "70"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For counting total number of Images in Folder\n",
    "count_Test = 0\n",
    "for i in X_Test:\n",
    "    count_Test+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "70"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TO storing the images pixels in a list and then converting the list into array.\n",
    "Xlist=[]\n",
    "for i in range(count):\n",
    "    Xlist.append(list(np.asarray(X_Train[i])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_TRAIN_MATRIX = np.asarray(Xlist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TO storing the images pixels in a list and then converting the list into array.\n",
    "Xlist=[]\n",
    "for i in range(count):\n",
    "    Xlist.append(list(np.asarray(X_Test[i])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_TEST_MATRIX = np.asarray(Xlist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# It will automatically read the path of CSV file.\n",
    "csv_path = os.getcwd() + \"\\\\dataset_USF_NonUSF_Train.csv\"\n",
    "csv = pd.read_csv(csv_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# It will take each image file's label(USF / Non USF) and store into a List.\n",
    "\n",
    "y = []\n",
    "for i in range(count):\n",
    "    x =csv['Label'][i]\n",
    "    y.append(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# It will convert the list into array.\n",
    "Y_TRAIN_MATRIX = np.asarray(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "# It will automatically read the path of CSV file.\n",
    "csv_path = os.getcwd() + \"\\\\dataset_USF_NonUSF_Test.csv\"\n",
    "csv = pd.read_csv(csv_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "# It will take each image file's label(USF / Non USF) and store into a List.\n",
    "\n",
    "y = []\n",
    "for i in range(count_Test):\n",
    "    x =csv['Label'][i]\n",
    "    y.append(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "# It will convert the list into array.\n",
    "Y_TEST_MATRIX = np.asarray(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(numpy.ndarray, numpy.ndarray, numpy.ndarray, numpy.ndarray)"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# To check the type of X_TRAIN_MATRIX,Y_TRAIN_MATRIX,X_TEST_MATRIX and Y_TEST_MATRIX.\n",
    "type(X_TRAIN_MATRIX),type(Y_TRAIN_MATRIX),type(X_TEST_MATRIX),type(Y_TEST_MATRIX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(350, 28, 28)"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# To check the shape of X_TRAIN_MATRIX.\n",
    "X_TRAIN_MATRIX.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(350,)"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# To check the shape of Y_TRAIN_MATRIX.\n",
    "Y_TRAIN_MATRIX.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(70, 28, 28)"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# To check the shape of X_TEST_MATRIX.\n",
    "X_TEST_MATRIX.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(70,)"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# To check the shape of Y_TEST_MATRIX.\n",
    "Y_TEST_MATRIX.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing Tensorflow packages\n",
    "import tensorflow as tf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_TRAIN_MATRIX = X_TRAIN_MATRIX / 255.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(350, 28, 28)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_TRAIN_MATRIX.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Building The Model:\n",
    "    # First layer will be flatten layer to flat the data\n",
    "    # Second layer will be Dense layer with 'Relu' activation layer\n",
    "    # We have taken 20% dropout\n",
    "    # Last dense layer with Softmax layer\n",
    "# After building, We compiled the Model.\n",
    "\n",
    "model = tf.keras.models.Sequential([\n",
    "  tf.keras.layers.Flatten(),\n",
    "  tf.keras.layers.Dense(512, activation=tf.nn.relu),\n",
    "  tf.keras.layers.Dropout(0.2),\n",
    "  tf.keras.layers.Dense(10, activation=tf.nn.softmax)\n",
    "])\n",
    "model.compile(optimizer='adam',\n",
    "              loss='sparse_categorical_crossentropy',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "350/350 [==============================] - 0s 300us/step - loss: 0.5157 - acc: 0.7971\n",
      "Epoch 2/30\n",
      "350/350 [==============================] - 0s 411us/step - loss: 0.5120 - acc: 0.8086\n",
      "Epoch 3/30\n",
      "350/350 [==============================] - 0s 479us/step - loss: 0.5066 - acc: 0.8057\n",
      "Epoch 4/30\n",
      "350/350 [==============================] - 0s 376us/step - loss: 0.5047 - acc: 0.7886\n",
      "Epoch 5/30\n",
      "350/350 [==============================] - 0s 459us/step - loss: 0.4991 - acc: 0.7886\n",
      "Epoch 6/30\n",
      "350/350 [==============================] - 0s 433us/step - loss: 0.5023 - acc: 0.8029\n",
      "Epoch 7/30\n",
      "350/350 [==============================] - 0s 405us/step - loss: 0.4967 - acc: 0.7857\n",
      "Epoch 8/30\n",
      "350/350 [==============================] - 0s 434us/step - loss: 0.4916 - acc: 0.8000\n",
      "Epoch 9/30\n",
      "350/350 [==============================] - 0s 462us/step - loss: 0.4903 - acc: 0.8086\n",
      "Epoch 10/30\n",
      "350/350 [==============================] - 0s 491us/step - loss: 0.4860 - acc: 0.8000\n",
      "Epoch 11/30\n",
      "350/350 [==============================] - 0s 510us/step - loss: 0.4827 - acc: 0.7971\n",
      "Epoch 12/30\n",
      "350/350 [==============================] - 0s 467us/step - loss: 0.4823 - acc: 0.8171\n",
      "Epoch 13/30\n",
      "350/350 [==============================] - 0s 448us/step - loss: 0.4777 - acc: 0.8143\n",
      "Epoch 14/30\n",
      "350/350 [==============================] - 0s 428us/step - loss: 0.4761 - acc: 0.8029\n",
      "Epoch 15/30\n",
      "350/350 [==============================] - 0s 455us/step - loss: 0.4734 - acc: 0.8029\n",
      "Epoch 16/30\n",
      "350/350 [==============================] - 0s 404us/step - loss: 0.4737 - acc: 0.8200\n",
      "Epoch 17/30\n",
      "350/350 [==============================] - 0s 439us/step - loss: 0.4679 - acc: 0.8114\n",
      "Epoch 18/30\n",
      "350/350 [==============================] - 0s 467us/step - loss: 0.4686 - acc: 0.8114\n",
      "Epoch 19/30\n",
      "350/350 [==============================] - 0s 476us/step - loss: 0.4674 - acc: 0.8114\n",
      "Epoch 20/30\n",
      "350/350 [==============================] - 0s 423us/step - loss: 0.4588 - acc: 0.8257\n",
      "Epoch 21/30\n",
      "350/350 [==============================] - ETA: 0s - loss: 0.4589 - acc: 0.843 - 0s 465us/step - loss: 0.4637 - acc: 0.8400\n",
      "Epoch 22/30\n",
      "350/350 [==============================] - 0s 484us/step - loss: 0.4639 - acc: 0.8114\n",
      "Epoch 23/30\n",
      "350/350 [==============================] - 0s 458us/step - loss: 0.4625 - acc: 0.8257\n",
      "Epoch 24/30\n",
      "350/350 [==============================] - 0s 429us/step - loss: 0.4601 - acc: 0.8029\n",
      "Epoch 25/30\n",
      "350/350 [==============================] - 0s 434us/step - loss: 0.4500 - acc: 0.8257\n",
      "Epoch 26/30\n",
      "350/350 [==============================] - 0s 472us/step - loss: 0.4509 - acc: 0.8286\n",
      "Epoch 27/30\n",
      "350/350 [==============================] - 0s 494us/step - loss: 0.4452 - acc: 0.8371\n",
      "Epoch 28/30\n",
      "350/350 [==============================] - 0s 425us/step - loss: 0.4416 - acc: 0.8429\n",
      "Epoch 29/30\n",
      "350/350 [==============================] - 0s 436us/step - loss: 0.4393 - acc: 0.8400\n",
      "Epoch 30/30\n",
      "350/350 [==============================] - 0s 479us/step - loss: 0.4383 - acc: 0.8457\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x1aadd172eb8>"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Now, fitting the model with no of epochs = 30\n",
    "model.fit(X_TRAIN_MATRIX, Y_TRAIN_MATRIX, epochs=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[12.433961268833706, 0.22857142899717603]"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Evaluating the model.\n",
    "model.evaluate(X_TEST_MATRIX, Y_TEST_MATRIX,verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving the model with .h5 extension.\n",
    "# once We will save this trained model then we can reuse it on other system.\n",
    "model.save_weights('myCNNModel.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
