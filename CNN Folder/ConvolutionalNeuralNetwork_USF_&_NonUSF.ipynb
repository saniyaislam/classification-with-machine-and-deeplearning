{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\asus\\Anaconda3\\Lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# In this line , We have imported all the packages that will be required further.\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import sys\n",
    "import inspect\n",
    "import datetime\n",
    "import matplotlib.pyplot as plt \n",
    "from datetime import datetime\n",
    "from scipy import misc\n",
    "from scipy import ndimage, misc\n",
    "from sklearn.datasets import fetch_mldata\n",
    "from PIL import Image\n",
    "import matplotlib\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Conv1D,Conv2D, MaxPooling2D\n",
    "from keras.layers import Flatten, Dense, Activation, Dropout\n",
    "#from keras import backend as K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is about getting the current directory path. \n",
    "# So, we are not required to chnage the path for dataset. It will automatically take the path for dataset.\n",
    "path = os.getcwd() + \"\\\\dataset_USF_NonUSF_Train\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is about getting the current directory path. \n",
    "# So, we are not required to chnage the path for dataset. It will automatically take the path for dataset.\n",
    "path1 = os.getcwd() + \"\\\\dataset_USF_NonUSF_Test\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\asus\\Anaconda3\\Lib\\site-packages\\ipykernel_launcher.py:9: DeprecationWarning: `imread` is deprecated!\n",
      "`imread` is deprecated in SciPy 1.0.0.\n",
      "Use ``matplotlib.pyplot.imread`` instead.\n",
      "  if __name__ == '__main__':\n",
      "C:\\Users\\asus\\Anaconda3\\Lib\\site-packages\\ipykernel_launcher.py:10: DeprecationWarning: `imresize` is deprecated!\n",
      "`imresize` is deprecated in SciPy 1.0.0, and will be removed in 1.2.0.\n",
      "Use ``skimage.transform.resize`` instead.\n",
      "  # Remove the CWD from sys.path while we load stuff.\n"
     ]
    }
   ],
   "source": [
    "# In this line of code, It will read the Images file from Train dataset and will resize each image file.\n",
    "# For resizing the image file, we are considering converting to 784 pixels each file, so that we can reduce the computation time.\n",
    "\n",
    "images = []\n",
    "X_Train = []\n",
    "for root, dirnames, filenames in os.walk(path):\n",
    "    for filename in filenames:\n",
    "        filepath = os.path.join(root, filename)\n",
    "        image = ndimage.imread(filepath, mode=\"1\")\n",
    "        image_resized = misc.imresize(image, (28,28))\n",
    "        X_Train.append(image_resized)\n",
    "        #image_resized = np.reshape(image_resized, (np.product(image_resized.shape)))\n",
    "        #images.append(image_resized) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\asus\\Anaconda3\\Lib\\site-packages\\ipykernel_launcher.py:9: DeprecationWarning: `imread` is deprecated!\n",
      "`imread` is deprecated in SciPy 1.0.0.\n",
      "Use ``matplotlib.pyplot.imread`` instead.\n",
      "  if __name__ == '__main__':\n",
      "C:\\Users\\asus\\Anaconda3\\Lib\\site-packages\\ipykernel_launcher.py:10: DeprecationWarning: `imresize` is deprecated!\n",
      "`imresize` is deprecated in SciPy 1.0.0, and will be removed in 1.2.0.\n",
      "Use ``skimage.transform.resize`` instead.\n",
      "  # Remove the CWD from sys.path while we load stuff.\n"
     ]
    }
   ],
   "source": [
    "# In this line of code, It will read the Images file from Test dataset and will resize each image file.\n",
    "# For resizing the image file, we are considering converting to 784 pixels each file, so that we can reduce the computation time.\n",
    "\n",
    "images = []\n",
    "X_Test = []\n",
    "for root, dirnames, filenames in os.walk(path1):\n",
    "    for filename in filenames:\n",
    "        filepath = os.path.join(root, filename)\n",
    "        image = ndimage.imread(filepath, mode=\"1\")\n",
    "        image_resized = misc.imresize(image, (28,28))\n",
    "        X_Test.append(image_resized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(350, 28, 28)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# To check the training dataset shape\n",
    "np.asarray(X_Train).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(70, 28, 28)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# To check the testing dataset shape\n",
    "np.asarray(X_Test).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For counting total number of Images in Folder\n",
    "count = 0\n",
    "for i in X_Train:\n",
    "    count+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "350"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For counting total number of Images in Folder\n",
    "count_Test = 0\n",
    "for i in X_Test:\n",
    "    count_Test+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "70"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TO storing the images pixels in a list and then converting the list into array.\n",
    "Xlist=[]\n",
    "for i in range(count):\n",
    "    Xlist.append(list(np.asarray(X_Train[i])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_TRAIN_MATRIX = np.asarray(Xlist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TO storing the images pixels in a list and then converting the list into array.\n",
    "Xlist=[]\n",
    "for i in range(count_Test):\n",
    "    Xlist.append(list(np.asarray(X_Test[i])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_TEST_MATRIX = np.asarray(Xlist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# It will automatically read the path of CSV file.\n",
    "csv_path = os.getcwd() + \"\\\\dataset_USF_NonUSF_Train.csv\"\n",
    "csv = pd.read_csv(csv_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# It will take each image file's label(USF / Non USF) and store into a List.\n",
    "\n",
    "y = []\n",
    "for i in range(count):\n",
    "    x =csv['Label'][i]\n",
    "    y.append(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# It will convert the list into array.\n",
    "Y_TRAIN_MATRIX = np.asarray(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# It will automatically read the path of CSV file.\n",
    "csv_path = os.getcwd() + \"\\\\dataset_USF_NonUSF_Test.csv\"\n",
    "csv = pd.read_csv(csv_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# It will take each image file's label(USF / Non USF) and store into a List.\n",
    "\n",
    "y = []\n",
    "for i in range(count_Test):\n",
    "    x =csv['Label'][i]\n",
    "    y.append(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# It will convert the list into array.\n",
    "Y_TEST_MATRIX = np.asarray(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(numpy.ndarray, numpy.ndarray, numpy.ndarray, numpy.ndarray)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# To check the type of X_TRAIN_MATRIX,Y_TRAIN_MATRIX,X_TEST_MATRIX and Y_TEST_MATRIX.\n",
    "type(X_TRAIN_MATRIX),type(Y_TRAIN_MATRIX),type(X_TEST_MATRIX),type(Y_TEST_MATRIX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(350, 28, 28)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# To check the shape of X_TRAIN_MATRIX.\n",
    "X_TRAIN_MATRIX.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(350,)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# To check the shape of Y_TRAIN_MATRIX.\n",
    "Y_TRAIN_MATRIX.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(70, 28, 28)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# To check the shape of X_TEST_MATRIX.\n",
    "X_TEST_MATRIX.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(70,)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# To check the shape of Y_TEST_MATRIX.\n",
    "Y_TEST_MATRIX.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing Tensorflow packages\n",
    "import tensorflow as tf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_TRAIN_MATRIX = X_TRAIN_MATRIX / 255.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(350, 28, 28)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_TRAIN_MATRIX.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Building The Model:\n",
    "    # First layer will be flatten layer to flat the data\n",
    "    # Second layer will be Dense layer with 'Relu' activation layer\n",
    "    # We have taken 20% dropout\n",
    "    # Last dense layer with Softmax layer\n",
    "# After building, We compiled the Model.\n",
    "\n",
    "model = tf.keras.models.Sequential([\n",
    "  tf.keras.layers.Flatten(),\n",
    "  tf.keras.layers.Dense(512, activation=tf.nn.relu),\n",
    "  tf.keras.layers.Dropout(0.2),\n",
    "  tf.keras.layers.Dense(10, activation=tf.nn.softmax)\n",
    "])\n",
    "model.compile(optimizer='adam',\n",
    "              loss='sparse_categorical_crossentropy',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "350/350 [==============================] - 0s 328us/step - loss: 0.2881 - acc: 0.8971\n",
      "Epoch 2/10\n",
      "350/350 [==============================] - 0s 331us/step - loss: 0.2735 - acc: 0.9171\n",
      "Epoch 3/10\n",
      "350/350 [==============================] - 0s 470us/step - loss: 0.2567 - acc: 0.9257\n",
      "Epoch 4/10\n",
      "350/350 [==============================] - 0s 425us/step - loss: 0.2463 - acc: 0.9086\n",
      "Epoch 5/10\n",
      "350/350 [==============================] - 0s 345us/step - loss: 0.2637 - acc: 0.9057\n",
      "Epoch 6/10\n",
      "350/350 [==============================] - 0s 416us/step - loss: 0.2057 - acc: 0.9314\n",
      "Epoch 7/10\n",
      "350/350 [==============================] - 0s 373us/step - loss: 0.2374 - acc: 0.9229\n",
      "Epoch 8/10\n",
      "350/350 [==============================] - 0s 373us/step - loss: 0.2072 - acc: 0.9429\n",
      "Epoch 9/10\n",
      "350/350 [==============================] - 0s 385us/step - loss: 0.1893 - acc: 0.9229\n",
      "Epoch 10/10\n",
      "350/350 [==============================] - 0s 387us/step - loss: 0.2661 - acc: 0.8971\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x28de3a26438>"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Now, fitting the model with no of epochs = 30\n",
    "model.fit(X_TRAIN_MATRIX, Y_TRAIN_MATRIX, epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluating the model.\n",
    "model.evaluate(X_TEST_MATRIX, Y_TEST_MATRIX,verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving the model with .h5 extension.\n",
    "# once We will save this trained model then we can reuse it on other system.\n",
    "model.save_weights('myCNNModel.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
